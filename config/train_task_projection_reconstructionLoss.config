[train] #train parameters
epoch = 100
batch_size = 16

reader_num = 0

optimizer = adamw
learning_rate = 0.001
weight_decay = 0
step_size = 1
lr_multiplier = 1


[distributed]
use = True
backend = nccl

[eval] #eval parameters
batch_size = 64

reader_num = 0

[dataset] #data parameters
#dataset = IMDB,laptop,MNLI,MRPC,QNLI,QQP,restaurant,RTE,SST2,STSB,WNLI 
dataset = IMDB,laptop,MNLI,MRPC,QNLI,QQP,restaurant,RTE,SST2,WNLI 

[model] #model parameters
model_type = Roberta

tqdm_ncols = 150
